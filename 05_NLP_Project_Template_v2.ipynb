{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now it's time to work on the final group project.  \n",
    "All of the pieces we've learned are integrated into this initial template to work on improving F1 measure on classification of pneumonia evidence\n",
    "\n",
    "## A few ideas of how to improve your system:\n",
    "* Improve targets.  Are there any False Negatives your system is missing?  Are there regular expressions that would help?\n",
    "* Improve modifiers.  Not all modifiers typically used in practice are the modifiers starter file.  Are there some to add?  Do some existing modifiers cause problems in your processing?  They can be changed or removed.\n",
    "* Improve document classification rules.  What rules work best?  What is the best \"default\" classification?\n",
    "* Consider handling of document \"sections\".  Are there certain headers or subsections which are more or less likely to contain evidence?  You could modify your own \"markup\" function to do this or you could add Modifiers to do this in some cases\n",
    "\n",
    "## Also before we get going, a few Pro Tips:\n",
    "* Remember that pyConText files need to be tab delimited.  If you edit these files in JupyterHub, it might be difficult to see the tabs and if you press \"TAB\" you will actually get spaces, so try to use Copy-and-Paste\n",
    "* Classification rules and modifiers are difficult.  Don't be afraid to ask for help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are going to use yaml for our knowledge representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!conda install pyyaml -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we continue we need to open our knowledge file(s) for editing.\n",
    "\n",
    "### In the File menu select open\n",
    "\n",
    "![opening a file](./opening_files.png)\n",
    "\n",
    "### cd to the KB file and select `pneumonia_targets.yml`\n",
    "\n",
    "![selecting yaml file](./select_yaml_file.png)\n",
    "\n",
    "### now open `pneumonia_targets.yml` to edit\n",
    "\n",
    "![editing yaml file](./edit_yaml_file.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import more_utils as mu\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading annotations from file : data/training_v2.zip\n",
      "Opening local file : data/training_v2.zip\n",
      "Total Annotated Documents : 70\n"
     ]
    }
   ],
   "source": [
    "annotated_doc_map = mu.read_doc_annotations('data/training_v2.zip')\n",
    "\n",
    "# let's also use a simple list of documents as well as this map\n",
    "annotated_docs = list(annotated_doc_map.values())\n",
    "\n",
    "print('Total Annotated Documents : {0}'.format(len(annotated_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our resources\n",
    "You're welcome to start new files or continue in the files we've used, but let's set up some defaults we have used in the course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETS_FILE_PATH = 'file:///' + os.path.join(os.getcwd(), 'KB/pneumonia_targets.yml')\n",
    "MODIFIERS_FILE_PATH = 'file:///' + os.path.join(os.getcwd(),'KB/pneumonia_modifiers.yml')\n",
    "CLASSIFIER_FILE_PATH = 'KB/classifierRules.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our targets and resources now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets loaded : 3\n",
      "Modifiers loaded : 363\n"
     ]
    }
   ],
   "source": [
    "# clear just in case files/regular expressions have been updated\n",
    "mu.clearPyConTextRegularExpressions()\n",
    "\n",
    "targets = mu.get_items(TARGETS_FILE_PATH)\n",
    "modifiers = mu.get_items(MODIFIERS_FILE_PATH)\n",
    "#targets = pyConTextNLP.itemData.instantiateFromCSVtoitemData(TARGETS_FILE_PATH)\n",
    "#modifiers = pyConTextNLP.itemData.instantiateFromCSVtoitemData(MODIFIERS_FILE_PATH)\n",
    "\n",
    "print('Targets loaded : {0}'.format(len(targets)))\n",
    "print('Modifiers loaded : {0}'.format(len(modifiers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct our Document Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debug_classifier = False\n",
    "docClassifier = mu.DocumentClassifier(CLASSIFIER_FILE_PATH, debug_classifier, modifiers, targets) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's attempt some predictions\n",
    "* You will do a lot of iterations modifying content and then coming back here to check performane\n",
    "* Remember : the prediction function passed here passes in a string (text) and returns a 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************\n",
      "Performance for Classifier :\n",
      "Precision : 0.9\n",
      "Recall :    0.7941176470588235\n",
      "F1:         0.84375\n",
      "\n",
      "Confusion Matrix : \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1\n",
       "Actual           \n",
       "0          33   3\n",
       "1           7  27"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('****************')\n",
    "print('Performance for Classifier :')\n",
    "mu.calculate_prediction_metrics(annotated_docs, docClassifier.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development of your system:\n",
    "* We have found the tools below for highlighting and graphing False Positives and False Negatives to be very useful.  We've provided them below in case it helps you as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE : You may need to modify this color mapping if you add  Target or Modifier categories not found here\n",
    "# prepare some colors for displaying any markup we might see\n",
    "colors = {\n",
    "    \"evidence_of_pneumonia\": \"orange\",\n",
    "    \"definite_negated_existence\": \"red\",\n",
    "    \"probable_negated_existence\": \"indianred\",\n",
    "    \"ambivalent_existence\": \"orange\",\n",
    "    \"probable_existence\": \"forestgreen\",\n",
    "    \"definite_existence\": \"green\",\n",
    "    \"historical\": \"goldenrod\",\n",
    "    \"indication\": \"pink\",\n",
    "    \"acute\": \"golden\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### We may need to download the textblob corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking up False Negatives\n",
      "Marking up False Positives\n",
      "Current total False Negatives : 7\n",
      "Current total False Positives : 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get our current set of false negatives and false positives if we use our simple toy classifier\n",
    "# which uses targets and a simplified implementation of modifiers\n",
    "current_false_negatives = mu.list_false_negatives(annotated_doc_map, docClassifier.predict)\n",
    "current_false_positives = mu.list_false_positives(annotated_doc_map, docClassifier.predict)\n",
    "\n",
    "fn_report_results = mu.marking_false_negatives(current_false_negatives, modifiers, targets)\n",
    "fp_report_results = mu.marking_false_positives(current_false_positives, modifiers, targets)\n",
    "\n",
    "print('Current total False Negatives : {0}'.format(len(current_false_negatives)))\n",
    "print('Current total False Positives : {0}'.format(len(current_false_positives)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For False Negatives, it's most useful to see the expert span annotations for positive pneumonia evidence to see if there may be targets that should be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b814fa9131b4e139d44bd1065ad21d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mu.view_pycontext_graph(fn_report_results, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For False Positives, it's most useful to see a pyConText graph since there may need to be modifiers adjusted so that targets can be properly utilized in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d6bd5e866b4f5dbef9178a47166e8b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mu.view_pycontext_graph(fp_report_results, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TEST SET Evaluation \n",
    "* We've been waiting for the test set.  It will not be available until the morning of the final class session.\n",
    "* At that time, you can uncomment this code and make any changes to it as instructed by the class instructors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_doc_map = mu.read_doc_annotations('data/training_v2.zip')\n",
    "\n",
    "# let's also use a simple list of documents as well as this map\n",
    "#test_docs = list(test_doc_map.values())\n",
    "\n",
    "#print('Total Test Documents : {0}'.format(len(test_docs)))\n",
    "\n",
    "# and now let's check performance on the TEST set...\n",
    "#print('****************')\n",
    "#print('Performance for Classifier on TEST set :')\n",
    "#mu.calculate_prediction_metrics(test_docs, docClassifier.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>This material presented as part of the DeCART Data Science for the Health Science Summer Program at the University of Utah in 2017.<br/>\n",
    "Presenters : Dr. Wendy Chapman, Jianlin Shi and Kelly Peterson"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
